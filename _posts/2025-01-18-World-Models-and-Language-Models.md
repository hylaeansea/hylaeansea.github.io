---
layout: post
title:  "World Models and Language Models, a Philosophy"
date:   2025-01-25 16:05:00 +0000
categories: blog
---

<style>
.prompt-box {
    border: 2px solidrgb(54, 56, 54); 
    padding: 10px; 
    border-radius: 5px; 
    background-color:rgb(215, 191, 255); 
    box-shadow: 2px 2px 5px rgba(0, 0, 1, 0.1);
}
.chatgpt-4o-box {
    border: 2px solidrgb(54, 56, 54); 
    padding: 10px; 
    border-radius: 5px; 
    background-color:rgb(202, 235, 194); 
    box-shadow: 2px 2px 5px rgba(0, 0, 1, 0.1);
}
.chatgpt-o1-box {
    border: 2px solidrgb(54, 56, 54); 
    padding: 10px; 
    border-radius: 5px; 
    background-color:rgb(159, 234, 140); 
    box-shadow: 2px 2px 5px rgba(0, 0, 1, 0.1);
}
.llama-box {
    border: 2px solidrgb(54, 56, 54); 
    padding: 10px; 
    border-radius: 5px; 
    background-color:rgb(209, 209, 209); 
    box-shadow: 2px 2px 5px rgba(0, 0, 1, 0.1);
}
.claude-box {
    border: 2px solidrgb(54, 56, 54); 
    padding: 10px; 
    border-radius: 5px; 
    background-color:rgb(187, 135, 116); 
    box-shadow: 2px 2px 5px rgba(0, 0, 1, 0.1);
}

</style>



# Abstract


![Computable Policy]({{ site.baseurl }}/assets/images/DALL·E_2025-01-18_11.27.13-A_futuristic_and_realistic_scene_depicting_the_concept_of_computable_policy.webp) 
*Image generated with DALL·E; prompt: make a futuristic but realistic image that exemplifies "computable policy" as you have just described*


# Introduction

Many of the incredible advances in AI over the last few years have come as the result of connecting external tools to large language models (LLMs). Tools in the form of:

- [OpenAI Code Interpreter and IDEs](https://platform.openai.com/docs/assistants/tools)
- [LangChain access to APIs](https://python.langchain.com/docs/introduction/)
- [Nvidia World Foundation Models](https://blogs.nvidia.com/blog/cosmos-world-foundation-models/)
- and many many others,

...provide LLMs with external simulations, application programming interfaces (APIs), data, and computation that doesn't have to be learned as part of the base model training or fine tuning. This allows for the LLM to access a much more diverse and rich set of data and ideas than it would otherwise have access to in it's training corpus. In fact, this idea of LLMs as a sort of "glue" for various tasks, data sets, and APIs form the foundational idea of Agentic AI. 

This approach however, has seen most of its success in domains that are readily computable such as access to APIs, physics simulations, or database queries. The very nature of "world models" is that their response can be calculated from physics models of the objects and agents in the virtual world. Another term for these are "Digital Twins" that simulate the behavior of a physical factory, smart city, or other physical system for example. I believe the reason for this rests in the fact that while they can be computationally expensive, we have really good mathematical models for many of these physical systems and thus they lend themselves to computation. This leads to my first conjecture about AIs and World Models: 

**Conjecture 1:** Large Language Models often excel at tasks where the accuracy of their output is relatively easy to verify. Testing whether code compiles, having a bilingual human verify a translation, determining if re-written prose sounds better, or a generated poem conforms to rhyme and meter, are all tasks that can be done by humans or verified computationally with relative ease. This makes reinforcement learning more tractable because we can plainly see (or test) if the output is correct.

However, there still exist a wide range of domains where it is more difficult, more subjective, and not as obvious to verify or test the correctness of an LLM output. It is not as easy, for example, to verify that a public policy recommendation is going to have the desired affect, or if a post on social media is going to foster civil discussion or cause harm, or if a particular social program is going to reach the desired people. These problems can't be "complied" as computer code can, these problems can sound coherent and well thought out, but then have secondary effects that are unintended. LLM generated public policy recommendations can be very detailed (especially with more and more detailed prompts as we will see below) and read very eloquently, however they will have been generated by "next token prediction" by a transformer network rather than generated by a simulation of, or contemplation and deliberation on, the outcome of the policy. Furthermore, if we can somehow make these foundations of society (law, policy, regulations, cultural text) more computable, we could auto generate massive amounts of training data to bake in this computable policy knowledge into the language models at training time. This could result in a future where our frontier AI models have computational knowledge of complex social, cultural, social, and civic domains. 

Therefore, the foundational motivation for this work is to answer the question:

***Can we make policy computable?***

# Prior Work

This idea of social, economic, policy, civic, and society simulation is not novel and there are many (many) examples where complex computational models have been used to guide public policy and decision making:

- *John Sterman* (2000). Business Dynamics: System Thinking and Modeling for a Complex World. New York: McGraw-Hill.
- *Stephen Wolfram (2016), [Computational Law, Symbolic Discourse and the AI Constitution](https://writings.stephenwolfram.com/2016/10/computational-law-symbolic-discourse-and-the-ai-constitution/)
- *Edward J Walters* (2019). [Data Driven Law](https://www.routledge.com/Data-Driven-Law-Data-Analytics-and-the-New-Legal-Services/Walters/p/book/9781498766654)
- [MIT Computable Law Project](https://www.media.mit.edu/articles/a-perspective-on-legal-algorithms/)

...and of course many others. So this idea is not new, and in fact has been pontificated on by some computing and legal luminaries. I do think though, that as AIs in general and LLMs in particular continue to find use in more and more domains in our society, that highlighting the particular shortcomings of the frontier models for policy generation, and providing a philosophy on how we can remedy them through this approach is important. Through this work I also want to explore two particular avenues that I think are relatively novel and warrant further work: 

1) **Can an LLM be used to create the "world model" and then by running the model, learn knowledge it wouldn't otherwise have?** That is, does an LLM have the internal knowledge to (with clever prompting) create the computational world model, create configurations of specific runs of the model, and then learn from the output? This would hint at "weak-to-strong-***self*** generalization" as an adaptation of [Weak-to-Strong Generalization](https://openai.com/index/weak-to-strong-generalization/) if the LLM can create, and then learn from a computational world model. This **self** generalization may have alignment implications as we more fully explore agentic systems. 

2) **Can we use computational policy and LLMs to design policy or ideas around increasing AI equality in our society?** 2025 is turning out to be a pivotal year for public policy and AI technological development each individually, but this might be an especially important year for the intersection of these two domains. The adoption, distribution, and development of AI systems by different slices of society in the near term, might have feedback effects that entrench these early distributions of the technology for a long time to come. It might be especially important then to leverage these rapidly advancing technologies to design AI policy and regulation that most closely align with the values of freedom, equality, and democracy. 

# Methodology

In this paper, I will explore these three questions: 

1. Can we make policy computable?
2. Can an LLM generate its own world model for policy computation? and 
3. Can we use LLMs and this generated world model to explore or design policy to increase the adoption of AI tech and hence digital and AI literacy and equality?

To explore these questions, I will break my investigation into the following steps:

1. First I will explore current AI models (gpt4o, claude 3.5 haiku, and ollama3.1:8b) capability in designing a policy to increase the adoption of AI. I will examine the qualitative features of the models at progressively higher levels of detail in the policy. This initial method of model comparison is illustrated in the following figure:

![LLM Policy Generation]({{ site.baseurl }}/assets/images/LLM_policy_generation_01.png)
*Figure 1: LLM Based Policy Generation and Iteration*

2. Next, I will also compare the model responses for different situations (AI education policy for two different municipalities) which I believe should cause substantive differences in the policy recommendations and outcomes. By comparing the different social situations, I will draw (again qualitative) comparisons between these. My hypothesis in this, is that while detailed and specific, there will be few differences between amongst the models for the same prompt, as well as few differences between the responses for the two municipalities. This method is illustrated below: 

![LLM Policy Generation]({{ site.baseurl }}/assets/images/LLM_policy_generation_02.png)
*Figure 2: LLM Based Policy Generation for Specific Municipalities*

3. Next, I will explore using a single LLM to generate a policy world model to simulate AI adoption policy impacts. For this I will be using a rather simple toy model developed in [agentpy](https://agentpy.readthedocs.io/en/latest/). Agentpy is a python module for simulating simple agent-based networks. I should note that these are not AI agents, but rather simple rules based agents. Even given this consideration, interesting and emergent behavior is often observed in models developed in agentpy. I will heavily leverage LLMs (specifically gpt4o and o1) in the creation of a progressively more complex agent based world model. This process is pictorially shown below.

![LLM generated world model]({{ site.baseurl }}/assets/images/LLM_generated_world_model_01.png)
*Figure 3: LLM Generated World Model*

4. After the world model is built, I will leverage LLMs to generate the starting conditions and parameters of the agentpy simulation, as well as interpret the numeric results in a qualitative natural language way. My hypothesis for this step is that the LLM is capable of generating an agent based world model simulation, from which it can gain knowledge that it doesn't have in its base model. This step is illustrated below:

![New Knowledge]({{ site.baseurl }}/assets/images/New_Knowledge.png)
*Figure 4: New Knowledge is potentially generated from the World Model interacting with the Language Model*


My hope is that through these steps I can demonstrate how LLMs can be used to not only generate rich world models for public policy recommendation and analysis, but also by using **AI tech adoption** as the exemplar policy in the above steps, develop some intuition about how we might adopt policy to increase that adoption. 

A quick disclaimer: I have only interest but no backround in public policy, or public tech policy. Furthermore, I will be exploring only toy models that are not fully descriptive of the complex dynamics, economics, and interactions in public policy analysis. Nevertheless, I think that this project serves as an interesting jumping off point for world model generation in this way, I if I can motivate the true public policy and social computing professionals to think about some of my proposed philosophy this will be a success. With that...*lets go!*

# Generating Public Policy with LLMs

In this first step I generated public policy recommendations at different levels of detail and evaluated the results for their qualitative strengths and weaknesses, as well as differences and similarities between models. As stated above, the paper will use as a use case, public policy to increase AI tech adoption across different cross sections of society. 

![LLM Policy Generation]({{ site.baseurl }}/assets/images/LLM_policy_generation_01.png)
*Figure 1: LLM Based Policy Generation and Iteration*

## High level National Policy

<div class="prompt-box">
<b>Prompt</b>: Generate a high level public policy that increases the use and adoption of AI across as many sectors of society as possible. Include specific recommendations and key points of the policy.
</div>
<br>

<div class="chatgpt-4o-box">
    <b>ChatGPT 4o</b>: Public Policy: "AI for All Initiative: Accelerating the Adoption of Artificial Intelligence Across Sectors"

<details>
  <summary>
    Click here for full text of response
  </summary>

### Public Policy: "AI for All Initiative: Accelerating the Adoption of Artificial Intelligence Across Sectors"

**Objective:**  
To maximize the adoption, responsible use, and benefits of artificial intelligence (AI) across all sectors of society while ensuring equitable access, economic growth, workforce preparedness, and ethical governance.

---

### **Key Policy Components:**

#### **1. Infrastructure Development**
- **AI Hubs:** Establish AI innovation hubs in key regions to provide centralized resources, training, and R&D facilities. Partner with universities, private organizations, and governments to build these hubs.
- **Cloud Computing Accessibility:** Subsidize access to high-performance computing (HPC) platforms, cloud AI services, and data storage for startups, SMEs, and public organizations.
- **5G and IoT Expansion:** Accelerate the deployment of 5G networks and IoT infrastructure to enhance real-time AI applications in areas like healthcare, transportation, and agriculture.

#### **2. Workforce and Education**
- **AI Education in Schools:** Introduce AI-focused curricula in schools starting at the primary level, emphasizing critical thinking, coding, and ethics.
- **Upskilling Programs:** Fund large-scale, government-sponsored upskilling and reskilling programs for workers in at-risk industries, focusing on AI literacy and related technical skills.
- **AI Fellowships and Grants:** Provide scholarships, fellowships, and grants for students and professionals pursuing careers in AI and adjacent fields.
- **Public AI Awareness Campaigns:** Run national campaigns to demystify AI, highlight its benefits, and build trust in AI systems.

#### **3. Sector-Specific Implementation Programs**
- **Healthcare:** Develop national frameworks for AI-driven healthcare systems, such as predictive diagnostics, personalized treatment plans, and telemedicine platforms.
- **Agriculture:** Promote AI for precision agriculture, crop monitoring, and resource optimization to enhance food security and reduce environmental impact.
- **Transportation:** Encourage the adoption of AI-powered systems for traffic management, autonomous vehicles, and public transit optimization.
- **Small Businesses:** Create an "AI Adoption Fund" offering grants and low-interest loans to SMEs for integrating AI into their operations.
- **Public Services:** Deploy AI tools for improving the efficiency of government services, public safety, and environmental monitoring.

#### **4. Ethical and Regulatory Frameworks**
- **AI Governance Standards:** Develop and enforce ethical AI principles focusing on transparency, fairness, accountability, and privacy.
- **Bias Mitigation:** Establish independent review boards to evaluate and mitigate AI bias in critical applications like hiring, lending, and law enforcement.
- **Data Privacy:** Strengthen data protection laws to ensure that individuals’ data is used ethically and securely in AI systems.
- **Explainable AI Mandates:** Require that high-impact AI applications provide transparent and explainable outputs.

#### **5. Research and Development (R&D)**
- **National AI R&D Fund:** Allocate a significant budget to fund breakthrough research in AI technologies, with a focus on societal benefit.
- **Public-Private Partnerships:** Foster collaborations between government, academia, and industry to accelerate AI innovation.
- **Open-Source AI:** Promote the development and adoption of open-source AI frameworks to democratize access and reduce barriers for innovators.

#### **6. International Collaboration**
- **Global AI Standards:** Work with international organizations to develop and adopt global AI standards for ethics, safety, and interoperability.
- **Knowledge Sharing:** Participate in global AI knowledge-sharing platforms and partnerships to access cutting-edge research and practices.
- **Technology Transfer:** Facilitate AI technology transfer to developing nations to ensure global equity in AI benefits.

#### **7. Incentives for Adoption**
- **Tax Benefits:** Offer tax credits and deductions for organizations adopting AI technologies or conducting AI R&D.
- **Recognition Programs:** Create awards and certifications for businesses and public organizations that demonstrate exemplary use of AI to solve societal challenges.
- **Procurement Policies:** Implement government procurement policies that prioritize vendors leveraging AI to improve efficiency and sustainability.

#### **8. Monitoring and Evaluation**
- **Impact Assessments:** Regularly assess the societal, economic, and environmental impact of AI deployment.
- **AI Readiness Index:** Publish an annual "AI Readiness Index" to track progress in AI adoption across sectors and regions.
- **Feedback Mechanisms:** Establish channels for public and industry feedback to continuously refine AI-related policies.

---

### **Expected Outcomes:**
- Enhanced productivity and innovation across all sectors.
- Creation of new industries and job opportunities, mitigating the risks of automation displacement.
- Improved public services, healthcare, and environmental sustainability.
- A globally competitive AI ecosystem with strong ethical foundations.

This policy framework aims to ensure that AI becomes a transformative tool for societal progress while addressing risks and ensuring inclusivity.
</details>

</div>


<br>
<div class="llama-box">
    <b>Llama3.1:8b</b>: Public Policy: "Accelerating the Adoption of Artificial Intelligence for Economic Growth and Social Progress"

<details>
  <summary>
    Click here for full text of response
  </summary>
  **Public Policy: "Accelerating the Adoption of Artificial Intelligence for Economic Growth and Social Progress"**

**Objective:** To encourage the widespread adoption and utilization of Artificial Intelligence (AI) across various sectors of society, fostering economic growth, improving social outcomes, and enhancing quality of life.

**Key Principles:**

1. **Invest in AI Education and Training**: Develop a comprehensive education system that equips students with AI literacy and programming skills, preparing them for the AI-driven workforce.
2. **Foster an Enabling Regulatory Environment**: Implement policies that facilitate AI innovation while ensuring data protection, privacy, and safety.
3. **Promote Public-Private Partnerships**: Encourage collaboration between governments, businesses, research institutions, and academia to accelerate AI adoption.
4. **Develop Infrastructure for AI Adoption**: Invest in high-speed computing infrastructure, data storage, and networking capabilities to support AI development and deployment.

**Recommendations:**

1. **Establish an AI Advisory Council**: Form a council comprising experts from various fields to provide guidance on AI policy development and implementation.
2. **Create an AI Innovation Fund**: Provide grants for businesses, startups, and research institutions to develop and deploy AI solutions addressing societal challenges.
3. **Implement AI-Specific Tax Incentives**: Offer tax breaks to companies investing in AI R&D, training programs, or job creation related to AI adoption.
4. **Develop AI-Related Workforce Development Programs**: Launch apprenticeships, vocational training, and certification programs for workers transitioning into AI-related roles.
5. **Conduct Regular AI Impact Assessments**: Monitor and evaluate the effects of AI on employment, skills, and industry competitiveness, informing policy adjustments as needed.

**Sector-Specific Recommendations:**

1. **Healthcare:** Invest in AI-powered healthcare platforms, electronic health records systems, and medical imaging analysis tools to improve patient outcomes and streamline care delivery.
2. **Transportation:** Promote the adoption of autonomous vehicles, traffic management systems, and smart logistics solutions to enhance safety, efficiency, and mobility.
3. **Education:** Develop AI-driven educational platforms for personalized learning, adaptive curricula, and virtual teaching assistants to support student success.
4. **Energy and Environment:** Leverage AI in renewable energy integration, energy efficiency monitoring, and environmental monitoring systems to mitigate climate change impacts.
5. **Agriculture:** Utilize precision agriculture techniques, AI-optimized crop management, and livestock tracking systems to boost food production, reduce waste, and enhance resource usage.

**Key Performance Indicators (KPIs):**

1. **AI Adoption Rate**: Monitor the number of businesses and individuals adopting AI solutions across various sectors.
2. **Job Creation in AI-Related Fields**: Track job growth in AI-related industries, such as data science, software engineering, and research.
3. **AI Research Output**: Measure the increase in peer-reviewed publications, patents, and innovations related to AI development.
4. **Economic Growth**: Assess the impact of AI adoption on GDP growth, productivity, and competitiveness.
5. **Social Outcomes**: Evaluate AI's effects on education outcomes, healthcare access, and environmental sustainability.

**Implementation Plan:**

1. **Short-Term (0-2 years)**:
	* Establish the AI Advisory Council
	* Launch AI Innovation Fund
	* Develop workforce development programs
2. **Medium-Term (2-5 years)**:
	* Implement tax incentives for AI-related investments
	* Conduct regular AI impact assessments
	* Develop sector-specific AI strategies and pilots
3. **Long-Term (5+ years)**:
	* Evaluate policy effectiveness and adjust as needed
	* Expand the AI Innovation Fund and workforce development programs
	* Establish a national AI strategy to guide future policy decisions

**Budget Allocation:**

1. **AI Education and Training**: 20%
2. **Regulatory Framework Development**: 15%
3. **Public-Private Partnerships**: 20%
4. **Infrastructure Development**: 25%
5. **Research and Development**: 10%

This comprehensive public policy aims to accelerate the adoption of AI across various sectors, addressing societal challenges while fostering economic growth and social progress.

</details>

</div>

<br>
<div class="claude-box">
    <b>Claude 3.5 Haiku</b>: National AI Integration and Development Policy

<details>
  <summary>
    Click here for full text of response
  </summary>
Here's a comprehensive AI Adoption and Integration Public Policy Framework:

# National AI Integration and Development Policy

## Core Objectives
- Accelerate responsible AI adoption across public and private sectors
- Develop robust national AI infrastructure and capabilities
- Create adaptive regulatory frameworks supporting innovation
- Ensure ethical and safe AI implementation

## Key Policy Recommendations

### 1. National AI Education and Workforce Development
- Mandate AI literacy and skills training in K-12 and higher education curricula
- Create federal grants for AI skills retraining programs
- Develop standardized AI competency certification frameworks

### 2. Research and Innovation Incentives
- Establish substantial research funding for AI technologies
- Provide tax credits for AI R&D investments
- Create public-private AI innovation partnerships
- Support emerging AI startups through dedicated venture funding

### 3. Sector-Specific AI Integration Strategies
- Healthcare: Accelerate AI diagnostic and treatment optimization
- Government: Implement AI for administrative efficiency and service delivery
- Manufacturing: Support AI-driven automation and process optimization
- Agriculture: Develop precision farming and crop management technologies
- Transportation: Advance autonomous vehicle and logistics technologies

### 4. Regulatory Framework
- Create adaptive AI governance structures
- Develop clear ethical guidelines for AI development
- Establish robust data privacy and algorithmic transparency standards
- Design risk assessment protocols for high-impact AI applications

### 5. Infrastructure Investment
- Build high-performance computing networks
- Develop secure, interoperable AI data platforms
- Support advanced semiconductor and computational infrastructure

### 6. International Collaboration
- Lead global AI standards and cooperation initiatives
- Facilitate international AI research exchanges
- Promote responsible AI development principles globally

### Implementation Mechanism
- Create a cross-ministerial AI Strategy Board
- Annual policy review and adaptation process
- Dedicated federal AI implementation budget

The policy aims to position the nation as a global leader in responsible, innovative AI integration across society.
</details>

</div>
<br>
Each model's response to this (admitedly) very high level prompt is available in the boxes above. Some of the main similarities include a focus on 1) education and upskilling, 2) governance, 3) incentives, and 4) infrastructure. These response are again, very high level, however they do provide some details on specific infrastructure that is needed, some specific details around budget (only llama3.1) and on technology emphasis. Further prompts (not all shown to keep this document manageable) about the reasons for the specific tech recomendations produced a couple interesting results:

* When further prompts to Claude asked why it chose to focus on HPC and if centralized or decentralized data centers were preferred, it replied:
<br>
<div class="claude-box">
    <b>Claude 3.5 Haiku</b>: Distributed HPC approach is superior. Reasons:

<details>
    <summary>
    Click here for full text of response
    </summary>
Distributed HPC approach is superior. Reasons:

1. Decentralization benefits
- Reduces single point of failure risks
- Democratizes computational access
- Increases overall national computational resilience

2. Local ecosystem advantages
- Universities can specialize in domain-specific research
- K-12 schools develop computational skills early
- Local research groups/communities drive innovation
- Reduces geographical computational inequality

3. Cost-effectiveness 
- Shared infrastructure reduces individual investment burdens
- Allows flexible, modular computational resource scaling
- Encourages collaborative computational resource sharing

Recommended strategy: Create national framework incentivizing distributed HPC network with standardized interoperability protocols, while providing funding and technical support for local infrastructure development.
</details>

</div>
<br>
* When further prompted about the most important piece of its policy, ChatGPT-4o replied:

<div class="chatgpt-4o-box">
    <b>ChatGPT 4o</b>: The **most important aspect** to focus on for maximizing AI adoption and unlocking its economic benefits is **education, awareness, and workforce readiness**. Here's why:

<details>
    <summary>
    Click here for full text of response
    </summary>


### **Why Focus on Education, Awareness, and Workforce Readiness?**
1. **Breaking Barriers to Adoption:**
   - Many people and organizations hesitate to adopt AI because they don’t understand it, feel intimidated, or perceive it as a threat to jobs.
   - By demystifying AI and building widespread AI literacy, individuals and organizations are more likely to adopt the technology.

2. **Empowering Workers and Businesses:**
   - Without a workforce equipped to use AI, even the most advanced technologies will remain underutilized.
   - Upskilling workers and supporting SMEs to integrate AI ensures broad economic participation and reduces fears of automation-driven unemployment.

3. **Creating Trust and Social Buy-In:**
   - A lack of trust in AI systems is a key adoption barrier. Public education campaigns that show tangible benefits of AI and address concerns (e.g., job displacement, privacy) build public confidence.

4. **Expanding the Talent Pool:**
   - Making AI education accessible increases the number of people who can develop, implement, and manage AI systems, ensuring a sustained pipeline of talent for economic growth.

---

### **Key Actions to Prioritize:**

#### **1. National AI Literacy Programs**
   - Launch free or low-cost online and offline AI training for all ages, from basic AI concepts to advanced technical skills.
   - Partner with tech companies and universities to create tailored content for diverse groups (e.g., students, business leaders, workers in at-risk industries).

#### **2. Upskilling the Workforce**
   - Provide government-funded, industry-aligned AI reskilling programs for workers in automation-prone sectors.
   - Offer incentives for businesses to retrain employees instead of replacing them with AI.

#### **3. AI Awareness Campaigns**
   - Run large-scale, engaging public campaigns showcasing real-world examples of AI improving lives (e.g., healthcare innovations, safer transportation, personalized education).
   - Emphasize how AI creates new job opportunities and reduces repetitive, mundane work.

#### **4. Support for Small Businesses and Entrepreneurs**
   - Launch "AI Starter Kits" for SMEs, including easy-to-use tools, training, and access to consultants, helping them integrate AI without requiring advanced technical expertise.
   - Subsidize AI implementation costs for small businesses to lower the financial barrier.

#### **5. Early AI Education**
   - Embed AI concepts into primary and secondary school curricula to prepare future generations.
   - Promote AI clubs, hackathons, and competitions for young learners to build interest and skills early.

---

### **Expected Results of This Focus:**
- **Broad Adoption:** A workforce and public that understand AI will be more willing and able to adopt it in their lives and workplaces.
- **Economic Growth:** Businesses that integrate AI see higher productivity, cost savings, and competitive advantages, which translate into economic gains at a national scale.
- **Social Equity:** Education and reskilling efforts ensure all communities, not just the affluent or urban ones, benefit from the AI revolution.

By focusing on **education, workforce readiness, and awareness**, this policy ensures that AI is not only accessible but embraced by as many people and sectors as possible.
</details>

</div>
<br>
* Llama3.1 was the only model to mention specific industries that would benefit from AI adoption, when asked which is the one that stands to gain the most, it replied:
<br>
<div class="llama-box">
    <b>Llama3.1:8b</b>: While AI has the potential to transform many industries, I'd argue that Healthcare stands to be transformed the most by increased adoption of AI. Here's why:

<details>
    <summary>
    Click here for full text of response
    </summary>
While AI has the potential to transform many industries, I'd argue that **Healthcare** stands to be transformed the most by increased adoption of AI. Here's why:

1. **Data-Driven Decision Making**: Healthcare generates vast amounts of data from electronic health records (EHRs), medical imaging, and wearable devices. AI can help analyze this data to identify patterns, predict patient outcomes, and inform treatment decisions.
2. **Personalized Medicine**: AI can help tailor treatments to individual patients based on their unique genetic profiles, medical histories, and lifestyle factors.
3. **Predictive Analytics**: AI-powered predictive models can forecast patient outcomes, allowing for proactive interventions and reduced hospital readmissions.
4. **Medical Imaging Analysis**: AI-assisted imaging analysis can improve diagnosis accuracy and reduce false positives/negatives in areas like cancer detection, cardiovascular disease, and neurology.
5. **Clinical Research**: AI can help accelerate clinical trials by identifying relevant patient populations, streamlining data collection, and predicting trial outcomes.
6. **Patient Engagement**: AI-powered chatbots and virtual assistants can enhance patient engagement, education, and adherence to treatment plans.
7. **Operational Efficiency**: AI can optimize hospital workflows, streamline supply chain management, and reduce administrative burdens.

**Potential AI Applications in Healthcare:**

1. **Computer-aided diagnosis (CAD) systems**: Automated image analysis for diagnosing diseases like cancer, diabetic retinopathy, or cardiovascular disease.
2. **Predictive modeling**: Forecasting patient outcomes, identifying high-risk patients, and optimizing resource allocation.
3. **Virtual nursing assistants**: AI-powered chatbots to support patient care, answer questions, and provide medication reminders.
4. **Personalized treatment planning**: AI-driven recommendations for tailored treatment plans based on individual patient characteristics.
5. **Clinical decision support systems (CDSS)**: Real-time advice for healthcare professionals on diagnosis, treatment, and patient care.

**Potential Benefits of Increased AI Adoption in Healthcare:**

1. **Improved patient outcomes**
2. **Reduced hospital readmissions**
3. **Enhanced clinical trial efficiency**
4. **Increased operational efficiency**
5. **Better resource allocation**
6. **Improved patient engagement and experience**

While AI has the potential to transform many industries, its impact on healthcare is likely to be particularly significant due to the vast amounts of data generated in this sector and the need for more efficient, effective, and personalized care delivery.
</details>

</div>
<br>
I would say that the prompts at this high level were fairly diverse. While they did have several similarities, each model had unique atributes that the others did not. This shouldn't be entirely surprising as even the same model with the same prompt can (and often do) give varying responses. This highlights one of the strengths of LLMs: to provide a jumping off point for further investigation. Lets go one level deeper; all of the models emphasized AI upskilling and education, lets dive into that.

## AI Up-skilling and Education

<div class="prompt-box">
    <b>Prompt</b>: You mention AI upskilling and education being a key component, can you further expand on 5 key aspects of your education policy? 
</div>
<br>
<div class="chatgpt-4o-box">
    <b>ChatGPT 4o</b>: Certainly! Expanding on the education policy for AI upskilling and literacy, here are **5 key aspects** to ensure inclusivity, effectiveness, and long-term impact:

<details>
    <summary>
    Click here for full text of response
    </summary>

### **1. Comprehensive AI Literacy for All Ages**
**Goal:** Build a society that understands and feels confident using AI.

- **School Curricula Integration:**  
  Introduce AI concepts progressively from primary school to high school:
  - Primary Level: Fun, interactive modules on problem-solving, logic, and data basics (e.g., identifying patterns).
  - Secondary Level: Coding, ethics of AI, and real-world AI applications like chatbots, robotics, or image recognition.
  - Tertiary Level: Advanced technical programs on machine learning, data science, and AI entrepreneurship.
  
- **Adult Education:**  
  Launch free or subsidized evening classes, workshops, and online courses tailored for adults in diverse sectors, ensuring they can use AI in their roles.

- **Accessible Learning Materials:**  
  Provide AI content in multiple languages, simplified for people with no technical background. Leverage engaging media formats like short videos, gamified apps, and interactive simulations.

---

### **2. Focused Workforce Reskilling Programs**
**Goal:** Enable workers in vulnerable industries to transition into AI-related roles or augment their current roles with AI skills.

- **Sector-Specific Programs:**  
  Create tailored AI training programs for industries like:
  - **Manufacturing:** AI-powered predictive maintenance and robotics operation.
  - **Healthcare:** AI-based diagnostics, patient management, and data analytics.
  - **Agriculture:** Precision farming using AI for yield optimization and pest control.

- **Microcredentialing:**  
  Offer short-term, stackable credentials (e.g., "AI for Business Managers," "AI in Retail") that workers can earn without committing to long, expensive degree programs.

- **Public-Private Partnerships:**  
  Collaborate with tech companies to co-design training programs and ensure skills taught match real-world demands.

---

### **3. AI Awareness Campaigns and Grassroots Initiatives**
**Goal:** Increase public trust in AI and dispel myths or fears.

- **Community AI Workshops:**  
  Partner with libraries, NGOs, and local governments to hold grassroots workshops in underserved areas, explaining AI in a relatable, non-technical way.

- **AI in Action Showcases:**  
  Create interactive exhibitions showcasing how AI solves real problems—e.g., detecting diseases, optimizing traffic, or protecting the environment.

- **Media Campaigns:**  
  Use TV, social media, and radio to run engaging campaigns, focusing on how AI empowers individuals (e.g., farmers using AI to predict weather) and creates opportunities.

---

### **4. Empowering Teachers and Educators**
**Goal:** Equip educators with the tools and knowledge to teach AI effectively.

- **Teacher Training Programs:**  
  Develop specific certifications for teachers in AI education, covering:
  - How to introduce AI concepts in the classroom.
  - Ethics and inclusivity in AI education.
  - Project-based learning for students (e.g., building simple AI models).

- **Ready-to-Use Resources:**  
  Provide schools and educators with ready-made lesson plans, activity kits, and free software platforms to reduce preparation time.

- **AI Mentorship Networks:**  
  Pair teachers with AI experts or professionals to guide them in developing content and running AI-related projects.

---

### **5. Accessible AI Learning Platforms**
**Goal:** Democratize access to AI learning resources, ensuring no one is left behind.

- **Open Educational Platforms:**  
  Create a free, government-supported AI education portal with tiered learning tracks:
  - Beginners: Basic literacy and fun AI experiments (e.g., training a chatbot).
  - Intermediate: Application of AI in various industries.
  - Advanced: Machine learning and algorithm design.

- **AI-Powered Personalized Learning:**  
  Use adaptive AI systems on these platforms to personalize lessons for users based on their learning pace and goals.

- **Hardware and Connectivity Support:**  
  Provide free or subsidized laptops, tablets, or internet access to underserved communities so they can access these programs.  

---

### **How This Policy Creates Impact**
1. **Widespread Literacy:** A foundational understanding of AI empowers individuals to adopt AI tools at work and home.
2. **Job Creation:** Reskilled workers can transition to higher-paying AI-related roles, fueling economic growth.
3. **Equity:** Accessible education reduces the digital divide, ensuring rural and underserved communities can benefit from the AI revolution.
4. **Trust:** Increased awareness and understanding reduce skepticism and resistance to AI adoption.
5. **Global Competitiveness:** A well-educated population positions the country as a leader in AI innovation and workforce readiness. 

By focusing on these five areas, the education policy ensures a holistic approach to AI adoption, addressing knowledge gaps and fostering a culture of innovation across all segments of society.
</details>

</div>
<br>
<div class="llama-box">
    <b>Llama3.1:8b</b>: Here are **5 Key Aspects** of an AI-focused Education Policy:

<details>
    <summary>
    Click here for full text of response
    </summary>

1. **AI Literacy for All Ages**:
	* Integrate basic AI concepts into school curricula (K-12) to ensure future generations understand the basics of AI.
	* Develop adult education programs and online courses to upskill workers in AI-related fields.
	* Make AI education accessible to underrepresented groups, such as women, minorities, and individuals with disabilities.
2. **STEM Education Enhancements**:
	* Strengthen core STEM subjects (math, science, technology, engineering) to provide a solid foundation for AI studies.
	* Incorporate project-based learning, coding, and robotics to foster hands-on experience with AI technologies.
	* Encourage interdisciplinary approaches to combine AI with other fields like social sciences, humanities, and arts.
3. **AI-Specific Training Programs**:
	* Develop specialized training programs for AI developers, data scientists, and AI engineers.
	* Offer certification programs in AI-related areas, such as natural language processing (NLP), computer vision, or machine learning (ML).
	* Collaborate with industry partners to create apprenticeships and on-the-job training opportunities.
4. **Continuous Learning and Upward Mobility**:
	* Foster a culture of lifelong learning by providing resources for continuous education and skill development.
	* Develop AI-focused career pathways and job descriptions to help workers navigate the evolving job market.
	* Encourage cross-functional collaboration between AI experts, business leaders, and educators to ensure AI adoption aligns with industry needs.
5. **Addressing Ethical and Societal Implications**:
	* Integrate ethics and societal implications of AI into education curricula to address concerns around bias, transparency, and accountability.
	* Encourage critical thinking, creativity, and human-centered design approaches in AI development and deployment.
	* Foster a deeper understanding of the impact of AI on society, including issues related to job displacement, digital divide, and cybersecurity.

**Key Features of an Effective Education Policy:**

1. **Collaboration**: Partner with industry leaders, educators, researchers, and policymakers to ensure education programs align with workforce needs.
2. **Flexibility**: Offer flexible learning options (online courses, part-time programs) to accommodate diverse learner needs.
3. **Interdisciplinary Approaches**: Combine AI with other disciplines to foster a more comprehensive understanding of its applications and limitations.
4. **Inclusivity**: Prioritize accessibility and diversity in education programs to ensure equal opportunities for all learners.
5. **Continuous Evaluation**: Regularly assess the effectiveness of education programs, making adjustments as needed to address emerging trends and challenges.

By incorporating these key aspects into an AI-focused Education Policy, we can equip future generations with the skills, knowledge, and critical thinking abilities necessary to thrive in an AI-driven world.
</details>

</div>

<br>
<div class="claude-box">
    <b>Claude 3.5 Haiku</b>: 5 Key AI Education Policy Aspects:

<details>
    <summary>
    Click here for full text of response
    </summary>


1. Curriculum Integration
- Embed AI literacy from elementary through higher education
- Develop age-appropriate computational thinking modules
- Teach AI ethics, capabilities, and societal implications alongside technical skills

2. Teacher Training & Development
- Comprehensive professional development programs
- Provide AI pedagogical resources and certification tracks
- Create instructor exchange programs with tech industry

3. Practical Skills Framework
- Hands-on AI project-based learning
- Establish student AI innovation challenge programs
- Partner with industries for real-world AI application experiences

4. Inclusive Access Strategy
- Provide computational resources in underserved communities
- Create scholarships targeting underrepresented groups in tech
- Develop free/low-cost online AI learning platforms

5. Continuous Learning Infrastructure
- Design lifelong AI skill update mechanisms
- Create micro-credentialing systems for AI competencies
- Develop adaptive learning pathways matching technological evolution

Core Philosophy: Transform education from passive technology consumption to active AI capability development.
</details>

</div>
<br>

Again there were some pretty strong similarities:
1. AI literacy and education across all ages, K-12, University, and Adult
2. A focus on reaching underserved groups and communities
3. Upskilling in the workplace as well as in school
4. Practical, hands on education
5. Addressing alignment, ethical, and social education

And some marked differences:
1. ChatGPT-4o provided much more detail including specific curriculum elements, specific workforce sectors (which is intresting as it did not mention specific sectors in the high level prompt), and highlights grass roots efforts
2. Llama3.1 mentions a direct integration of AI ethics into the curriculum rather than a standalone element.
3. Claude 3.5 was rather brief with its response - This may have been due to the fact that the time of day of my inquiry was a high demand time and I did get a message that the responses would be made brief.

Again, the responses at this level were similar in some respects, however each one emphasized a different aspect of the policy. As an interesting aside, I wonder if in an agentic system that leverages several diverse LLMs to produce responses to a fairly complex prompt (like these), if the similarities can be viewed as a sort of voting mechanism to elicit the high level ideas that are the biggest priority. For example, if all three mention education and specifically life long education, maybe that can be used as an indication of where to expend resources. *Perhaps the topic of another paper*

Lets go 1 level more detailed in our prompt, but also try to elicit some more specifics about two diverse municipalities.

## Differences in Municipalities

<div class="prompt-box">
<b>Prompt</b>: Provide a concise but specific education plan for AI adoption in K-12 in the Boulder Valley School District.
</div>
<br>

<div class="chatgpt-4o-box">
    <b>ChatGPT 4o</b>: 

<details>
    <summary>
    Click here for full text of response
    </summary>
### AI Education Plan for K-12 in Boulder Valley School District (BVSD)

**Objective:** To integrate AI literacy into the K-12 curriculum, ensuring students develop critical thinking, creativity, and ethical awareness for an AI-driven future.

---

### **1. Curriculum Integration**
- **Elementary School (K-5):**  
  - **Focus:** Introduction to basic concepts of AI through interactive, play-based learning.  
  - **Activities:**  
    - Pattern recognition using visual games (e.g., teaching a “robot” to sort colors/shapes).  
    - Storytelling with AI-powered tools (e.g., co-creating stories using AI writing assistants).  
    - Ethics through storytelling: Discuss how AI can help (e.g., predicting weather) but also make mistakes.  

- **Middle School (6-8):**  
  - **Focus:** Hands-on exposure to AI applications and fundamental computational thinking.  
  - **Modules:**  
    - Basics of machine learning: Using simple tools like Teachable Machine by Google to train models.  
    - Coding fundamentals: Introduce Python via interactive platforms like Scratch-to-Python or Code.org.  
    - AI ethics discussions: Explore privacy, bias, and fairness in AI systems (e.g., facial recognition).  
    - AI in daily life: Analyze case studies like smart assistants, recommendation systems, or self-driving cars.  

- **High School (9-12):**  
  - **Focus:** Advanced AI concepts, coding skills, and industry-relevant applications.  
  - **Electives and Pathways:**  
    - **AI and Data Science Elective:** Machine learning models, data analysis with Python, and AI toolkits (e.g., TensorFlow Lite).  
    - **Interdisciplinary Projects:** AI applied to climate change, healthcare, or robotics using Arduino kits.  
    - **Capstone AI Projects:** Students develop their own AI solutions (e.g., creating chatbots or predictive apps).  
    - **Ethics in AI:** A dedicated module on the societal impacts of AI, exploring regulation and responsibility.

---

### **2. Teacher Training and Support**
- Provide BVSD teachers with annual workshops on integrating AI topics into math, science, and social studies.  
- Offer AI-focused professional development through partnerships with local universities like CU Boulder.  
- Create an online resource hub with lesson plans, AI tools, and case studies tailored to different grade levels.

---

### **3. Extracurricular Programs**
- **AI Clubs:** Launch AI and robotics clubs in every middle and high school, supported by local tech companies.  
- **Competitions:** Organize district-wide hackathons and challenges focused on AI for social good.  
- **Summer Camps:** Run AI-focused summer programs where students can work on real-world projects with mentors.  

---

### **4. Community and Industry Partnerships**
- Partner with CU Boulder and local tech companies to provide guest lectures, mentorship, and project collaboration opportunities.  
- Host community workshops for parents and students to build awareness of AI's benefits and challenges.  

---

### **5. Equity and Accessibility**
- Ensure all BVSD schools have access to the necessary technology (e.g., laptops, internet).  
- Provide AI learning materials in multiple languages and formats to support diverse learners.  
- Target outreach and scholarships for underserved students to participate in advanced AI programs.

---

### **Timeline**
1. **Year 1:**  
   - Develop and pilot elementary and middle school AI curricula in a subset of schools.  
   - Train 50% of BVSD educators on AI basics and classroom tools.  
2. **Year 2-3:**  
   - Expand the program district-wide with full integration into science and computer science courses.  
   - Launch AI electives and capstone projects in all high schools.  
3. **Year 4:**  
   - Measure student outcomes, refine the curriculum, and deepen partnerships with local universities and industry.  

---

**Outcome:** By 2030, BVSD graduates will enter higher education and the workforce with a solid foundation in AI, fostering innovation and responsible AI use in society.
</details>

</div>

<br>
<div class="llama-box">
    <b>Llama3.1:8b</b>: 

<details>
    <summary>
    Click here for full text of response
    </summary>
**Boulder Valley School District AI Education Plan (K-12)**

**Grade-Level Breakdown:**

1. **Elementary (K-5)**:
	* Introduce basic AI concepts through interactive games and simulations
	* Develop critical thinking, problem-solving, and coding skills using block-based programming languages like Scratch or Blockly
2. **Middle School (6-8)**:
	* Build on elementary foundations with more advanced coding concepts and AI-related topics (e.g., machine learning, natural language processing)
	* Encourage students to design and develop their own AI projects using platforms like Code.org or Google's CS First
3. **High School (9-12)**:
	* Offer specialized courses in AI and related fields (e.g., data science, computer science)
	* Collaborate with local industry partners to provide real-world case studies and guest lectures

**Course Curriculum:**

1. **"Introduction to Artificial Intelligence" (Grades 6-8)**:
	* Explore basic AI concepts, including machine learning and neural networks
	* Develop critical thinking and problem-solving skills through project-based learning
2. **"AI and Data Science" (Grades 9-12)**:
	* Delve deeper into AI topics like natural language processing, computer vision, and robotics
	* Use real-world datasets and tools to develop data analysis and visualization skills
3. **"Artificial Intelligence in Industry" (Grades 11-12)**:
	* Explore AI applications in various industries (e.g., healthcare, finance, transportation)
	* Collaborate with industry partners on case studies and projects

**Teacher Professional Development:**

1. **AI-focused workshops and training sessions**
2. **Online courses and certifications** (e.g., Coursera, edX)
3. **Collaboration with local universities and research institutions**

**Infrastructure and Resources:**

1. **Establish a district-wide AI lab** for hands-on learning and project development
2. **Provide access to industry-standard software and tools** (e.g., TensorFlow, PyTorch)
3. **Develop an online platform for students to share their projects and collaborate with peers**

**Assessment and Evaluation:**

1. **Regular assessments and quizzes** to evaluate student understanding of AI concepts
2. **Project-based evaluations** to assess critical thinking, problem-solving, and coding skills
3. **Industry partnerships** to provide real-world feedback on student projects

**Timeline:**

* **Year 1-2**: Develop and implement AI curriculum in middle school grades (6-8)
* **Year 3-4**: Expand AI education to high school grades (9-12) and develop specialized courses
* **Year 5+**: Continuously evaluate and improve the AI education program, exploring new technologies and applications

This plan provides a concise but specific outline for introducing AI education in K-12. By following this roadmap, the Boulder Valley School District can ensure that students are well-prepared to thrive in an increasingly AI-driven world.
</details>

</div>

<br>
<div class="claude-box">
    <b>Claude 3.5 Haiku</b>: 

<details>
    <summary>
    Click here for full text of response
    </summary>
Boulder Valley School District AI Education Adoption Plan:

## K-12 AI Integration Framework

### Elementary Level (K-5)
- Computational thinking modules
- Age-appropriate AI concept introduction
- Basic coding and algorithmic reasoning skills
- Ethical technology use foundations

### Middle School (6-8)
- Introductory machine learning concepts
- Block-based programming platforms
- AI project-based learning challenges
- Digital citizenship and AI ethics curriculum

### High School (9-12)
- Advanced AI/ML programming courses
- Python and data science foundations
- Industry partnership internship programs
- AI application development projects
- Ethical AI design and societal impact studies

### Infrastructure Requirements
- 1:1 device program with computational capabilities
- Cloud-based learning platforms
- Professional development for educators
- Dedicated AI curriculum specialists

### Implementation Milestones
- Year 1: Curriculum development
- Year 2: Pilot programs in select schools
- Year 3: District-wide implementation
- Year 4: Continuous improvement and expansion

Targeted outcome: Students as AI-literate, ethically-aware technology creators, not just consumers.
</details>

</div>
<br>
<div class="prompt-box">
<b>Prompt</b>: Provide a concise but specific education plan for AI adoption in K-12 in the Denver Public School District.
</div>
<br>

<div class="chatgpt-4o-box">
    <b>ChatGPT 4o</b>: 

<details>
    <summary>
    Click here for full text of response
    </summary>
### AI Education Plan for K-12 in Denver Public School District (DPS)

**Objective:** Equip DPS students with foundational AI literacy, critical thinking, and practical skills to thrive in an AI-powered world, emphasizing inclusivity and community engagement.

---

### **1. Curriculum Integration**
- **Elementary School (K-5):**
  - **Focus:** Early exposure to AI concepts and computational thinking.
  - **Activities:**
    - Introduce pattern recognition and problem-solving using games and hands-on activities.
    - Use simple AI-powered tools (e.g., drawing apps with AI suggestions) for creative projects.
    - Teach ethics and teamwork through stories about AI helping solve real-world problems (e.g., weather forecasting, recycling).

- **Middle School (6-8):**
  - **Focus:** Hands-on exploration of AI tools and their real-world applications.
  - **Modules:**
    - Train simple AI models using platforms like Google's Teachable Machine.
    - Explore coding basics (e.g., Python or Blockly) in the context of AI.
    - Discuss AI ethics: privacy, bias, and fairness in everyday AI systems like social media algorithms.
    - Real-world projects: Students create AI-powered tools for local community challenges (e.g., energy conservation apps).

- **High School (9-12):**
  - **Focus:** Advanced technical skills, project-based learning, and career pathways.
  - **Electives and Programs:**
    - **AI & Machine Learning Elective:** Teach data analysis, neural networks, and programming with tools like TensorFlow or PyTorch.
    - **Interdisciplinary Projects:** Apply AI to climate change, urban planning, or health through robotics, drones, or IoT sensors.
    - **Capstone Projects:** Students design AI-driven solutions for local issues (e.g., predictive traffic apps or personalized learning tools).
    - **AI Ethics and Policy:** Explore societal impacts and policy debates around AI technologies.

---

### **2. Teacher Training and Resources**
- **Professional Development:** Offer annual AI-focused training workshops for DPS teachers, emphasizing integration into STEM and social sciences.
- **Resource Hub:** Create a DPS-specific online portal with ready-to-use AI lesson plans, tools, and case studies.
- **Mentorship Program:** Partner with local AI professionals and university experts (e.g., University of Denver) to support teachers.

---

### **3. Extracurricular and Enrichment Programs**
- **AI Clubs:** Establish AI and robotics clubs in middle and high schools, fostering peer learning and innovation.
- **Competitions:** Host district-wide AI challenges focused on solving Denver-specific issues, like air quality or urban mobility.
- **Summer Camps:** Offer AI-focused summer enrichment programs for all grade levels, supported by community partners.
- **AI Showcases:** Hold annual exhibitions where students demonstrate their AI projects to the community.

---

### **4. Partnerships with Local Institutions**
- Partner with organizations like the University of Denver, Colorado School of Mines, and local tech companies to provide guest lectures, internships, and project mentorship.
- Collaborate with Denver-based non-profits and city officials to align student projects with community needs.

---

### **5. Equity and Accessibility**
- **Technology Access:** Ensure every DPS student has access to devices, software, and reliable internet for AI learning.
- **Inclusive Materials:** Provide AI content in multiple languages, with culturally relevant examples and support for neurodiverse learners.
- **Targeted Outreach:** Focus on underserved schools and neighborhoods with scholarships for advanced AI programs.

---

### **Timeline**
1. **Year 1:** 
   - Pilot AI modules in 10 elementary, middle, and high schools.
   - Train 25% of DPS teachers on AI concepts and tools.
2. **Year 2-3:**
   - Roll out AI-focused curriculum district-wide.
   - Launch AI clubs and competitions in all middle and high schools.
   - Deepen partnerships with local universities and tech companies.
3. **Year 4-5:**
   - Expand advanced AI electives and capstone projects district-wide.
   - Measure outcomes and refine curricula based on community feedback.

---

**Outcome:** By 2030, DPS will produce AI-literate graduates equipped with technical, ethical, and problem-solving skills, ready to contribute to Denver's economy and beyond.
</details>

</div>

<br>
<div class="llama-box">
    <b>Llama3.1:8b</b>: 

<details>
    <summary>
    Click here for full text of response
    </summary>
**Denver Public Schools (DPS) AI Education Plan (K-12)**

**Grade-Level Breakdown:**

1. **Elementary (K-5)**:
	* "Introduction to Coding and AI" (Grades 3-5)
		+ Teach basic coding concepts using block-based programming languages like Code.org or Scratch
		+ Introduce AI-related topics, such as machine learning and robotics
2. **Middle School (6-8)**:
	* "AI and Computer Science" (Grades 6-8)
		+ Develop more advanced coding skills using Python or Java
		+ Explore AI applications in areas like natural language processing and computer vision
3. **High School (9-12)**:
	* "Artificial Intelligence and Data Science" (Grades 9-12)
		+ Offer specialized courses in AI, data science, and computer science
		+ Collaborate with industry partners to provide real-world case studies and guest lectures

**Course Curriculum:**

1. **"Introduction to Artificial Intelligence" (Grades 6-8)**:
	* Explore basic AI concepts, including machine learning and neural networks
	* Develop critical thinking and problem-solving skills through project-based learning
2. **"AI in Industry" (Grades 9-12)**:
	* Delve deeper into AI applications in various industries (e.g., healthcare, finance, transportation)
	* Use real-world datasets and tools to develop data analysis and visualization skills

**Teacher Professional Development:**

1. **AI-focused workshops and training sessions**
2. **Online courses and certifications** (e.g., Coursera, edX)
3. **Collaboration with local universities and research institutions**

**Infrastructure and Resources:**

1. **Establish a district-wide AI lab** for hands-on learning and project development
2. **Provide access to industry-standard software and tools** (e.g., TensorFlow, PyTorch)
3. **Develop an online platform for students to share their projects and collaborate with peers**

**Assessment and Evaluation:**

1. **Regular assessments and quizzes** to evaluate student understanding of AI concepts
2. **Project-based evaluations** to assess critical thinking, problem-solving, and coding skills
3. **Industry partnerships** to provide real-world feedback on student projects

**Timeline:**

* **Year 1-2**: Develop and implement AI curriculum in middle school grades (6-8)
* **Year 3-4**: Expand AI education to high school grades (9-12) and develop specialized courses
* **Year 5+**: Continuously evaluate and improve the AI education program, exploring new technologies and applications

**Denver Public Schools' Commitment:**

1. **Invest $500,000 in AI-related infrastructure and resources**
2. **Hire an AI Education Specialist to lead district-wide initiatives**
3. **Collaborate with local businesses and organizations to provide industry partnerships and guest lectures**

By following this plan, Denver Public Schools can ensure that students are well-prepared for the challenges of an increasingly AI-driven world.
</details>

</div>

<br>
<div class="claude-box">
    <b>Claude 3.5 Haiku</b>: 

<details>
    <summary>
    Click here for full text of response
    </summary>
Denver Public Schools AI Education Implementation Plan

### Core Objectives
- Comprehensive AI literacy across K-12
- Equitable technological access
- Ethical AI skill development

### K-5 Foundation
- Computational thinking integration
- Basic coding exposure
- Digital citizenship fundamentals

### 6-8 Middle School Progression
- Introduction to machine learning
- Block-based programming
- Algorithmic reasoning skills

### 9-12 High School Advanced Track
- Python programming
- Data science foundations
- AI project development
- Industry partnership opportunities

### Key Implementation Strategies
- 1:1 device program
- Teacher AI certification
- Curriculum aligned with industry standards
- Focus on underserved community engagement

### Resource Requirements
- $3.5M initial investment
- 12 dedicated AI curriculum specialists
- Professional development budget
- Technology infrastructure upgrades

Targeted Outcome: Prepare students as AI-capable, ethically-informed innovators.

</details>
</div>

Again we can see distinct differences, but strong similarities between both the models for the same school district as well as the same model for different school districts. This is a fairly strong confirmation of my hypothesis that however complex the transformer networks are, next token prediction fails to consider the nuanced differences between policy tailored for AI promotion in fairly different school districts. This leads to our next investigation of AI generated world models; my hope is that we can elicit some of this nuance by building world models of these social and economic scenarios, even if they are simple models.

# Modeling of AI Uptake

With the qualitative analysis of policy generation under out belts, we can now explore the generation of models of AI technology adoption and interfacing with these models. Again this will be limited to just this one policy case (AI uptake), and again will just start with a the simplest possible model with a couple of complications. We will use the agentpy framework for this as described above, and will also expand our use of models to use ChatGPT-o1. We will use as a reference Rodgers' diffusion of innovation model, which is a simple model that can be used to explore the dynamics of adoption of new technologies.

<div class="prompt-box">
<b>Prompt</b>: Can you write some python code using agentpy for a simple implementation of rodgers' diffusion of innovation?

</div>
<br>

<div class="chatgpt-o1-box">
    <b>ChatGPT o1</b>: 
<details>
    <summary>
    Click here for full text of response
    </summary>
    <pre>
    <code>
import agentpy as ap
import matplotlib.pyplot as plt
import numpy as np

class InnovationAgent(ap.Agent):
    def setup(self):
        self.adopted = False
        self.category = category_distribution(self.id, self.model.p.population)

    def adopt_innovation(self):
        if not self.adopted:
            if self.category == 'innovator':
                self.adopted = True
            elif self.category in ['early_adopter', 'early_majority', 'late_majority']:
                neighbors = self.model.network.neighbors(self)
                adopters = sum(1 for n in neighbors if n.adopted)
                threshold = self.model.thresholds[self.category]
                if len(neighbors) > 0 and adopters / len(neighbors) >= threshold:
                    self.adopted = True
            elif self.category == 'laggard':
                neighbors = self.model.network.neighbors(self)
                adopters = sum(1 for n in neighbors if n.adopted)
                if len(neighbors) > 0 and adopters / len(neighbors) >= self.model.thresholds['laggard']:
                    self.adopted = True

class DiffusionModel(ap.Model):
    def setup(self):
        self.agents = ap.AgentList(self, self.p.population, InnovationAgent)
        self.network = ap.Network(agents=self.agents, connections=self.p.avg_degree)
        self.network.add_edges(self.agents)
        self.agents.setup()

        self.category_distribution = self.p.category_distribution
        self.thresholds = {
            'innovator': self.p.innovator_threshold,
            'early_adopter': self.p.early_adopter_threshold,
            'early_majority': self.p.early_majority_threshold,
            'late_majority': self.p.late_majority_threshold,
            'laggard': self.p.laggard_threshold
        }

    def step(self):
        for agent in self.agents:
            agent.adopt_innovation()

    def update(self):
        adopted_agents = sum(1 for a in self.agents if a.adopted)
        self.record('Adopted', adopted_agents)

    def end(self):
        self.record('Final Adoption Rate', sum(1 for a in self.agents if a.adopted) / self.p.population)

def category_distribution(agent_id, population):
    categories = ['innovator', 'early_adopter', 'early_majority', 'late_majority', 'laggard']
    probabilities = [0.05, 0.1, 0.35, 0.35, 0.15]
    assignment = np.random.choice(categories, p=probabilities, size=population)
    return assignment[agent_id]

parameters = ap.Sample({
    'population': 100,
    'avg_degree': 4,
    'steps': 20,
    'category_distribution': category_distribution,
    'innovator_threshold': 0.0,
    'early_adopter_threshold': 0.1,
    'early_majority_threshold': 0.3,
    'late_majority_threshold': 0.5,
    'laggard_threshold': 0.8
})

exp = ap.Experiment(DiffusionModel, parameters, iterations=1)
results = exp.run()

plt.plot(results['t'], results['Adopted'])
plt.title('Diffusion of Innovation Over Time')
plt.xlabel('Time Steps')
plt.ylabel('Number of Adopters')
plt.show()

</code>
</pre>

</details>
</div>

This code has all the elements to run a simple network model where agents adopt technology according to 1) the category that they are in and 2) the percentage of their network connections that have adopted the technology.

A few iterations on pandas errors and indexing errors and we eventually arrived at this.

<details>
    <summary>
    Click here for full text of response
    </summary>
<pre>
<code>
# Model design
import agentpy as ap
import networkx as nx
import numpy as np
import random 

# Visualization
import matplotlib.pyplot as plt 
import seaborn as sns
import IPython

class InnovationAgent(ap.Agent):
    def setup(self):
        """
        Initializes the agent's adoption state and assigns it to a category based on predefined probabilities.
        """
        # Everyone starts as having not adopted the technology
        self.adopted = False

        # Select a category for the agent using the given probabilities
        rng = np.random
        self.category_idx = rng.choice(len(self.p.categories), p=self.p.probabilities)
        self.category = self.p.categories[self.category_idx]

    def num_adopted_neighbors(self):
        """
        Calculates the number of neighboring agents who have adopted the innovation.
        Returns:
            int: The count of neighbors who have adopted.
        """
        # Get the neighbors of the current agent
        neighbors = self.network.neighbors(self)
        
        # Check adoption status for each neighbor
        adopted = [n.adopted for n in neighbors]
        
        # Count the number of adopters
        return sum(adopted)

    def neighbor_adopted_frac(self):
        """
        Calculates the fraction of neighboring agents who have adopted the innovation.
        Returns:
            float: Fraction of neighbors who have adopted, or 0 if there are no neighbors.
        """
        # Get the neighbors of the current agent
        neighbors = self.network.neighbors(self)
        
        # Check adoption status for each neighbor
        adopted = [n.adopted for n in neighbors]
        len_neighbors = len(adopted)

        # Handle case where there are no neighbors
        if len_neighbors == 0:
            return 0

        # Calculate and return the fraction of adopters
        num_adopters = sum(adopted)
        return num_adopters / len_neighbors

    def adopt_innovation(self):
        """
        Determines whether the agent adopts the innovation based on its category
        and the adoption status of its neighbors.
        """
        # Only proceed if the agent has not already adopted the innovation
        if not self.adopted:
            # Innovators adopt unconditionally
            if self.category == 'innovator':
                self.adopted = True
            # Other categories adopt based on a threshold of neighbor adoption
            elif self.category in ['early_adopter', 'early_majority', 'late_majority', 'laggard']:
                # Calculate the fraction of neighbors who have adopted
                adopted_frac = self.neighbor_adopted_frac()

                # Get the threshold for the agent's category
                threshold = self.p.thresholds[self.category_idx]

                # Adopt if the fraction meets or exceeds the threshold
                if adopted_frac >= threshold:
                    self.adopted = True

class InnovationModel(ap.Model):
    """
    This class represents the overall model for innovation diffusion. It manages the setup,
    simulation steps, and final results.
    """

    def setup(self):
        """
        Initializes the network and agents for the simulation. A small-world network is
        created using the Watts-Strogatz model, and agents are assigned to it.
        """
        graph = nx.watts_strogatz_graph(
            self.p.population,
            self.p.number_of_connections,
            self.p.network_rewire_probability,
        )

        # Create agents and a network
        self.agents = ap.AgentList(self, self.p.population, InnovationAgent)
        self.network = self.agents.network = ap.Network(self, graph)
        self.network.add_agents(self.agents, self.network.nodes)

    def update(self):
        """
        Calculates the fraction of agents who have adopted the innovation and records it.
        Stops the simulation if all agents have adopted.
        """
        frac_adopted = sum(1 for a in self.agents if a.adopted) / self.p.population
        self.record("fraction adopted", frac_adopted)

        # Stop if everyone has adopted the tech
        if frac_adopted == 1:
            self.stop()

    def step(self):
        """
        Executes a single step of the simulation. During each step, agents decide
        whether to adopt the innovation based on their category and network connections.
        """
        for agent in self.agents:
            agent.adopt_innovation() # Agents adopt innovations based on their category and network connections
    
    def end(self):
        """
        Finalizes the simulation. Additional computations or outputs can be added here if needed.
        """
        pass



parameters = {
    'population': 100,
    "categories": ['innovator', 'early_adopter', 'early_majority', 'late_majority', 'laggard'],
    "probabilities": [0.05, 0.1, 0.35, 0.35, 0.15],
    "thresholds": [0.0, 0.1, 0.25, 0.5, 0.9],
    "number_of_connections": 4,
    "network_rewire_probability": 0.1,
    "steps":100,
}



model = InnovationModel(parameters)
results = model.run()

data = results.variables.InnovationModel
ax = data.plot()


</code>
</pre>

</details>

With a fairly boring plot of tech adoption vs simulation step.

I next prompted o1 with ideas on ways to expand the model, and to be honest, I cant track down exactly the difference between modifications I made to the code vs what o1 suggested. After a handful of iterations we ended with the following expansions of the model and agent code:

<details>
    <summary>
    Click here for full text of response
    </summary>
<pre>
<code>
# Model design
import agentpy as ap
import networkx as nx
import numpy as np
import random 

# Visualization
import matplotlib.pyplot as plt 
import seaborn as sns
import IPython

class InnovationAgent(ap.Agent):
    def setup(self):
        """
        Initializes the agent's adoption state, assigns it to a category, and sets its budget.
        """
        self.adopted = False

        # Assign category and probabilities
        rng = np.random
        self.category_idx = rng.choice(len(self.p.categories), p=self.p.probabilities)
        self.category = self.p.categories[self.category_idx]

        # Assign random budget for the agent
        self.money = rng.uniform(0, 200)  # Example range

    def can_use_ai(self):
        """
        Checks if the agent can use AI resources based on their money and availability.
        Returns:
            bool: True if the agent can use AI, False otherwise.
        """
        return (self.money >= self.p.ai_cost) and (self.model.ai_resources > 0)

    def num_adopted_neighbors(self):
        """
        Counts the number of neighboring agents who have adopted the innovation.
        """
        neighbors = self.network.neighbors(self)
        return sum(n.adopted for n in neighbors)

    def neighbor_adopted_frac(self):
        """
        Calculates the fraction of neighboring agents who have adopted the innovation.
        Returns:
            float: Fraction of adopters or 0 if there are no neighbors.
        """

        # Get the neighbors of the current agent
        neighbors = self.network.neighbors(self)
        
        # Check adoption status for each neighbor
        adopted = [n.adopted for n in neighbors]
        len_neighbors = len(adopted)

        # Handle case where there are no neighbors
        if len_neighbors == 0:
            return 0

        # Calculate and return the fraction of adopters
        num_adopters = sum(adopted)
        return 1.0 * num_adopters / len_neighbors

    def adopt_use_contribute(self):
        """
        Handles the agent's behavior: adopting, using, or contributing to AI resources.
        """
        # Add money probabilistically every turn
        
        # self.money += np.random.uniform(1,1.1)
        self.money += np.random.uniform(self.p.standard_money_gain_min, self.p.standard_money_gain_max)
        
        # decide whether to use
        if self.adopted and self.can_use_ai():
            self.use_technology()
        
        # Decide whether to contribute
        if ((np.random.uniform() < self.p.contributor_fraction)) or (self.category == "innovator"):
            self.contribute_to_ai()

        if not self.adopted:
            # Innovators adopt unconditionally if they can afford AI
            if self.category == 'innovator' and self.can_use_ai():
                self.adopted = True

            # Other categories adopt based on a threshold and affordability
            elif self.category in ['early_adopter', 'early_majority', 'late_majority', 'laggard']:
                adopted_frac = self.neighbor_adopted_frac()
                threshold = self.p.thresholds[self.category_idx]

                if adopted_frac >= threshold and self.can_use_ai():
                    self.adopted = True
                    self.use_technology()

    def use_technology(self):
        """
        Deducts the cost of using AI and reduces global AI resources.
        """
        # print("used ai resource")
        self.money -= self.p.ai_cost
        self.model.ai_resources -= 1

    def contribute_to_ai(self):
        """
        Generates and contributes additional money and AI resources to the system.
        """
        generated_money = np.random.uniform(self.p.ai_money_gain_min, self.p.ai_money_gain_max)
        generated_resources = np.random.randint(self.p.resource_gain_min, self.p.resource_gain_max + 1)
        # print("{} resources added".format(generated_resources))

        self.money += generated_money
        self.model.ai_resources += generated_resources

def gini(x):

    """ Calculate Gini Coefficient """
    # By Warren Weckesser https://stackoverflow.com/a/39513799
    
    x = np.array(x)
    mad = np.abs(np.subtract.outer(x, x)).mean()  # Mean absolute difference
    rmad = mad / np.mean(x)  # Relative mean absolute difference
    return 0.5 * rmad 

class InnovationModel(ap.Model):
    """
    This class represents the overall model for innovation diffusion. It manages the setup,
    simulation steps, and final results.
    """

    def setup(self):
        """
        Initializes the network and agents for the simulation. A small-world network is
        created using the Watts-Strogatz model, and agents are assigned to it.
        """
        graph = nx.watts_strogatz_graph(
            self.p.population,
            self.p.number_of_connections,
            self.p.network_rewire_probability,
        )

        # Create agents and a network
        self.agents = ap.AgentList(self, self.p.population, InnovationAgent)
        self.network = self.agents.network = ap.Network(self, graph)
        self.network.add_agents(self.agents, self.network.nodes)

        # Initialize AI resources
        self.ai_resources = self.p.total_ai_resources

    def update(self):
        """
        Calculates the fraction of agents who have adopted the innovation and records it.
        Stops the simulation if all agents have adopted.
        """
        num_adopted = sum(1 for a in self.agents if a.adopted)
        frac_adopted = num_adopted / self.p.population

        self.record("num_adopted", num_adopted)
        self.record("fraction_adopted", frac_adopted)

        remaining_resources = self.ai_resources
        total_money = sum(agent.money for agent in self.agents)

        # Record key metrics
        self.record('remaining_resources', remaining_resources)
        self.record('total_money', total_money)
        self.record('Gini Coefficient', gini(self.agents.money))

        
        # Stop if everyone has adopted the tech
        if frac_adopted == 1:
            self.stop()

    def step(self):
        """
        Executes a single step of the simulation. During each step, agents decide
        whether to adopt the innovation based on their category and network connections.
        """
        for agent in self.agents:
            agent.adopt_use_contribute() # Agents adopt innovations based on their category and network connections
    
    def end(self):
        """
        Finalizes the simulation. Additional computations or outputs can be added here if needed.
        """
        pass

parameters = {
    'population': 10000,
    'categories': ['innovator', 'early_adopter', 'early_majority', 'late_majority', 'laggard'],
    'contributor_fraction':0.00, # % of adoptors contribute to tech
    'probabilities': [0.05, 0.1, 0.35, 0.35, 0.15],
    'thresholds': [0.0, 0.01, 0.25, 0.5, 0.9],
    'number_of_connections': 20,
    'network_rewire_probability': 0.05,

    'total_ai_resources': 30,  # Total AI resources available
    'ai_cost': 10,  # Cost of using AI for each adoption
    'standard_money_gain_min': -2, # Minimum money generated by normal economic output
    'standard_money_gain_max': 4, # Maximum money generated by normal economic output
    'ai_money_gain_min': 0,  # Minimum money generated by AI usage
    'ai_money_gain_max': 50,  # Maximum money generated by AI usage
    'resource_gain_min': 0,  # Minimum AI resources generated by adoption
    'resource_gain_max': 3,  # Maximum AI resources generated by adoption

    'steps': 250,
}

model = InnovationModel(parameters)
results = model.run()

data = results.variables.InnovationModel
# ax = data["Gini Coefficient"].plot()
# ax = data["fraction_adopted"].plot()
# ax = data["num_adopted"].plot()
ax = data["remaining_resources"].plot()
# ax = data["total_money"].plot()
# plt.close()
</code>
</pre>

</details>

This is a more complicated model that allows us to examine other quantitative feature of the model including gini coeficient. This then gives us a world model that we can prompt with different configuration parameters, as well as leverage the llm to evaluate the output.

# Discussion

It is clear from the above exploration, that 1) LLMs are an imensely powerful tool to generate ideas and jumping off points for public policy, and 2) world models can be generated from LLMs that can in turn be used to generate new knowledge for the given domain. As I have stated multiple times in this document, I am not a public policy expert, however I hope this investigation serves to stir ideas on new ways to generate and analyze policy by those who are.

## What does this mean for safety

There is intense debate in the AI safety community as to whether increasing the capability of AIs is better for AI safety and alignment, or if with it comes inherant danger. I believe this is as yet un-settled and still a matter of taste and philosophy. With that being said, I believe that increasing the training corpus of LLMs with data from progressively more realistic policy, economic and agent models, will result in a general decrease in the risk posed by these models, for this purpose. Furthermore, if we can lower the barier, and through this investigation, increase the awareness of the benefits, of language model and world model integration, this should generally increase the number of practicioners of both AI computing and public policy generating these models and evaluating their performance. And with increased participation, education, and awareness, comes an inherant increase in safety.

# Future Work

If it isn't obvious, this paper fell far short of my intended goal. Not to dispare though, that leaves pleanty of interesting work, and interesting questions for me, (and hopefully you!) to answer in the future. These include:

1. Can we quantify the amount of knowldege gained from the World Model generation?
2. Can we standardize suggested prompts for generating either public policy suggestions, or world models of said policy?
3. Can the input (parameter) output (results) pairs of numeric data be transformed into natural language descriptions for fine tuning or base model training?

I also set out to investigate other public policy topics including Ocean Biodiversity and Conservation, Public Heath/epidemiology policy, and Space Development Policy. However, if I am not an expert in tech pedagogy policy then I am certainly not an expert in these fields. I do however think that these fields being at the intersection of technology, public policy, and economics might prove for interesting case studies.

# Closing

To conclude, I think the jury is still out on computable policy: I showed that advanced language models can create progressively more detailed policy recomendations through progressively more detailed prompting. And that LLMs can generate world models that can run simulations of different aspects of a given policy. However, I think this work falls short of making policy truely computable. Despite this shortcoming, I hope this work contributes to your mental model of Language Model and World Model interactions. Furthermore, if you are so inclined, please critique, critisize, augment, implement, or extend this work to make it more detailed and rich. That would have made this worth it. 

Thank you to the people at Blue Dot Impact, facilitators, moderators, and all of those in my cohort, this was a great course and you all are doing great work. Thank you to 80,000 Hours, your work is changing peoples careers and hopefully the course of humanity for the long term. Thank you to my local AI chat group, your questions and insights are thought provoking and fun. And finally, thank you to my family, for giving me this time to pursue this interest of mine, and for putting up with the clacking of the mechanical keyboard in the kitchen.

Cheers,
Kyle

